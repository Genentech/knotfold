{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc5-mbsX9PZC"
      },
      "source": [
        "# KnotFold Colab\n",
        "\n",
        "This Colab notebook builds off the [AlphaFold v2.3.2 notebook](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) to extend to the algorithm described in the associated KnotFold manuscript.\n",
        "\n",
        "After predicting the baseline AlphaFold structure, the KnotFold extension allows you to specify a desired disulfide connectivity of the peptide of interest.\n",
        "\n",
        "The same caveats explained in the original notebook (lack of using templates and smaller MSAs due to a reduced BFD database) apply to this notebook.\n",
        "\n",
        "\n",
        "Any publication that discloses findings arising from using this notebook should cite the [KnotFold paper](link) and should [cite](https://github.com/deepmind/alphafold/#citing-this-work) the [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC1dKAwk2eyl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the 2 cells below to set up AlphaFold and all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "woIxeCPygt7K"
      },
      "outputs": [],
      "source": [
        "# Set environment variables before running any other code.\n",
        "import os\n",
        "os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '4.0'\n",
        "\n",
        "#@title 1. Install third-party software\n",
        "\n",
        "#@markdown Please execute this cell by pressing the _Play_ button\n",
        "#@markdown on the left to download and import third-party software\n",
        "#@markdown in this Colab notebook. (See the [acknowledgements](https://github.com/deepmind/alphafold/#acknowledgements) in our readme.)\n",
        "\n",
        "#@markdown **Note**: This installs the software on the Colab\n",
        "#@markdown notebook in the cloud and not on your computer.\n",
        "\n",
        "from IPython.utils import io\n",
        "import os\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      # Uninstall default Colab version of TF.\n",
        "      %shell pip uninstall -y tensorflow keras\n",
        "\n",
        "      %shell sudo apt install --quiet --yes hmmer\n",
        "      pbar.update(6)\n",
        "\n",
        "      # Install py3dmol.\n",
        "      %shell pip install py3dmol\n",
        "      pbar.update(2)\n",
        "\n",
        "      # Install OpenMM and pdbfixer.\n",
        "      %shell rm -rf /opt/conda\n",
        "      %shell wget -q -P /tmp \\\n",
        "        https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "          && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "          && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "      pbar.update(9)\n",
        "\n",
        "      PATH=%env PATH\n",
        "      %env PATH=/opt/conda/bin:{PATH}\n",
        "      %shell conda install -qy conda==24.1.2 \\\n",
        "          && conda install -qy -c conda-forge \\\n",
        "            python=3.10 \\\n",
        "            openmm=7.7.0 \\\n",
        "            pdbfixer\n",
        "      pbar.update(80)\n",
        "\n",
        "      # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "      %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "      %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "      pbar.update(2)\n",
        "\n",
        "      %shell wget -q -P /content \\\n",
        "        https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "      pbar.update(1)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "executed_cells = set([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VzJ5iMjTtoZw"
      },
      "outputs": [],
      "source": [
        "#@title 2. Download AlphaFold\n",
        "\n",
        "#@markdown Please execute this cell by pressing the *Play* button on\n",
        "#@markdown the left.\n",
        "\n",
        "GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-12-06.tar'\n",
        "PARAMS_DIR = './alphafold/data/params'\n",
        "PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "\n",
        "try:\n",
        "  with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      %shell rm -rf alphafold\n",
        "      %shell git clone --branch main {GIT_REPO} alphafold\n",
        "      pbar.update(8)\n",
        "      # Install the required versions of all dependencies.\n",
        "      %shell pip3 install -r ./alphafold/requirements.txt\n",
        "      # Run setup.py to install only AlphaFold.\n",
        "      %shell pip3 install --no-dependencies ./alphafold\n",
        "      %shell pip3 install pyopenssl==22.0.0\n",
        "      pbar.update(10)\n",
        "\n",
        "      # Make sure stereo_chemical_props.txt is in all locations where it could be searched for.\n",
        "      %shell mkdir -p /content/alphafold/alphafold/common\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/alphafold/common\n",
        "      %shell mkdir -p /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "      %shell cp -f /content/stereo_chemical_props.txt /opt/conda/lib/python3.10/site-packages/alphafold/common/\n",
        "\n",
        "      # Load parameters\n",
        "      %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "      %shell wget -O \"{PARAMS_PATH}\" \"{SOURCE_URL}\"\n",
        "      pbar.update(27)\n",
        "\n",
        "      %shell tar --extract --verbose --file=\"{PARAMS_PATH}\" \\\n",
        "        --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "      %shell rm \"{PARAMS_PATH}\"\n",
        "      pbar.update(55)\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "import jax\n",
        "if jax.local_devices()[0].platform == 'tpu':\n",
        "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "elif jax.local_devices()[0].platform == 'cpu':\n",
        "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "else:\n",
        "  print(f'Running with {jax.local_devices()[0].device_kind} GPU')\n",
        "\n",
        "# Make sure everything we need is on the path.\n",
        "import sys\n",
        "sys.path.append('/opt/conda/lib/python3.10/site-packages')\n",
        "sys.path.append('/content/alphafold')\n",
        "\n",
        "executed_cells.add(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4JpOs6oA-QS"
      },
      "source": [
        "## Making a prediction\n",
        "\n",
        "Please paste the sequence of your protein in the text box below, then run the remaining cells via _Runtime_ > _Run after_. You can also run the cells individually by pressing the _Play_ button on the left.\n",
        "\n",
        "Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rowN0bVYLe9n",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 3. Enter the amino acid sequence(s) to fold ⬇️\n",
        "#@markdown Enter the amino acid sequence(s) to fold:\n",
        "\n",
        "from alphafold.notebooks import notebook_utils\n",
        "# Track cell execution to ensure correct order.\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 3)\n",
        "\n",
        "import enum\n",
        "\n",
        "@enum.unique\n",
        "class ModelType(enum.Enum):\n",
        "  MONOMER = 0\n",
        "  MULTIMER = 1\n",
        "\n",
        "sequence_1 = 'GCPRILMRCKQDSDCLAGCVCGPNGFCGSP'  #@param {type:\"string\"}\n",
        "\n",
        "input_sequences = (sequence_1, )\n",
        "\n",
        "MIN_PER_SEQUENCE_LENGTH = 16\n",
        "MAX_PER_SEQUENCE_LENGTH = 4000\n",
        "MAX_MONOMER_MODEL_LENGTH = 2500\n",
        "MAX_LENGTH = 4000\n",
        "MAX_VALIDATED_LENGTH = 3000\n",
        "\n",
        "use_multimer_model_for_monomers = False\n",
        "\n",
        "# Validate the input sequences.\n",
        "sequences = notebook_utils.clean_and_validate_input_sequences(\n",
        "    input_sequences=input_sequences,\n",
        "    min_sequence_length=MIN_PER_SEQUENCE_LENGTH,\n",
        "    max_sequence_length=MAX_PER_SEQUENCE_LENGTH)\n",
        "\n",
        "if len(sequences) == 1:\n",
        "  if use_multimer_model_for_monomers:\n",
        "    print('Using the multimer model for single-chain, as requested.')\n",
        "    model_type_to_use = ModelType.MULTIMER\n",
        "  else:\n",
        "    print('Using the single-chain model.')\n",
        "    model_type_to_use = ModelType.MONOMER\n",
        "else:\n",
        "  print(f'Using the multimer model with {len(sequences)} sequences.')\n",
        "  model_type_to_use = ModelType.MULTIMER\n",
        "\n",
        "# Check whether total length exceeds limit.\n",
        "total_sequence_length = sum([len(seq) for seq in sequences])\n",
        "if total_sequence_length > MAX_LENGTH:\n",
        "  raise ValueError('The total sequence length is too long: '\n",
        "                   f'{total_sequence_length}, while the maximum is '\n",
        "                   f'{MAX_LENGTH}.')\n",
        "\n",
        "# Check whether we exceed the monomer limit.\n",
        "if model_type_to_use == ModelType.MONOMER:\n",
        "  if len(sequences[0]) > MAX_MONOMER_MODEL_LENGTH:\n",
        "    raise ValueError(\n",
        "        f'Input sequence is too long: {len(sequences[0])} amino acids, while '\n",
        "        f'the maximum for the monomer model is {MAX_MONOMER_MODEL_LENGTH}. You may '\n",
        "        'be able to run this sequence with the multimer model by selecting the '\n",
        "        'use_multimer_model_for_monomers checkbox above.')\n",
        "\n",
        "if total_sequence_length > MAX_VALIDATED_LENGTH:\n",
        "  print('WARNING: The accuracy of the system has not been fully validated '\n",
        "        'above 3000 residues, and you may experience long running times or '\n",
        "        f'run out of memory. Total sequence length is {total_sequence_length} '\n",
        "        'residues.')\n",
        "\n",
        "executed_cells.add(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tTeTTsLKPjB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4. Search against genetic databases\n",
        "\n",
        "#@markdown Once this cell has been executed, you will see\n",
        "#@markdown statistics about the multiple sequence alignment\n",
        "#@markdown (MSA) that will be used by AlphaFold. In particular,\n",
        "#@markdown you’ll see how well each residue is covered by similar\n",
        "#@markdown sequences in the MSA.\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 4)\n",
        "\n",
        "# --- Python imports ---\n",
        "import collections\n",
        "import copy\n",
        "from concurrent import futures\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "from urllib import request\n",
        "from google.colab import files\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "\n",
        "from alphafold.model import model\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "\n",
        "from alphafold.data import feature_processing\n",
        "from alphafold.data import msa_pairing\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.data import pipeline_multimer\n",
        "from alphafold.data.tools import jackhmmer\n",
        "\n",
        "from alphafold.common import confidence\n",
        "from alphafold.common import protein\n",
        "\n",
        "from alphafold.relax import relax\n",
        "from alphafold.relax import utils\n",
        "\n",
        "from IPython import display\n",
        "from ipywidgets import GridspecLayout\n",
        "from ipywidgets import Output\n",
        "\n",
        "output_dir = 'prediction'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Color bands for visualizing plddt\n",
        "PLDDT_BANDS = [(0, 50, '#FF7D45'),\n",
        "               (50, 70, '#FFDB13'),\n",
        "               (70, 90, '#65CBF3'),\n",
        "               (90, 100, '#0053D6')]\n",
        "\n",
        "# --- Find the closest source ---\n",
        "test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2022_01.fasta.1'\n",
        "ex = futures.ThreadPoolExecutor(3)\n",
        "def fetch(source):\n",
        "  request.urlretrieve(test_url_pattern.format(source))\n",
        "  return source\n",
        "fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "source = None\n",
        "for f in futures.as_completed(fs):\n",
        "  source = f.result()\n",
        "  ex.shutdown()\n",
        "  break\n",
        "\n",
        "JACKHMMER_BINARY_PATH = '/usr/bin/jackhmmer'\n",
        "DB_ROOT_PATH = f'https://storage.googleapis.com/alphafold-colab{source}/latest/'\n",
        "# The z_value is the number of sequences in a database.\n",
        "MSA_DATABASES = [\n",
        "    {'db_name': 'uniref90',\n",
        "     'db_path': f'{DB_ROOT_PATH}uniref90_2022_01.fasta',\n",
        "     'num_streamed_chunks': 62,\n",
        "     'z_value': 144_113_457},\n",
        "    {'db_name': 'smallbfd',\n",
        "     'db_path': f'{DB_ROOT_PATH}bfd-first_non_consensus_sequences.fasta',\n",
        "     'num_streamed_chunks': 17,\n",
        "     'z_value': 65_984_053},\n",
        "    {'db_name': 'mgnify',\n",
        "     'db_path': f'{DB_ROOT_PATH}mgy_clusters_2022_05.fasta',\n",
        "     'num_streamed_chunks': 120,\n",
        "     'z_value': 623_796_864},\n",
        "]\n",
        "\n",
        "# Search UniProt and construct the all_seq features only for heteromers, not homomers.\n",
        "if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "  MSA_DATABASES.extend([\n",
        "      # Swiss-Prot and TrEMBL are concatenated together as UniProt.\n",
        "      {'db_name': 'uniprot',\n",
        "       'db_path': f'{DB_ROOT_PATH}uniprot_2021_04.fasta',\n",
        "       'num_streamed_chunks': 101,\n",
        "       'z_value': 225_013_025 + 565_928},\n",
        "  ])\n",
        "\n",
        "TOTAL_JACKHMMER_CHUNKS = sum([cfg['num_streamed_chunks'] for cfg in MSA_DATABASES])\n",
        "\n",
        "MAX_HITS = {\n",
        "    'uniref90': 10_000,\n",
        "    'smallbfd': 5_000,\n",
        "    'mgnify': 501,\n",
        "    'uniprot': 50_000,\n",
        "}\n",
        "\n",
        "\n",
        "def get_msa(sequences):\n",
        "  \"\"\"Searches for MSA for given sequences using chunked Jackhmmer search.\n",
        "\n",
        "  Args:\n",
        "    sequences: A list of sequences to search against all databases.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary mapping unique sequences to dicionaries mapping each database\n",
        "    to a list of  results, one for each chunk of the database.\n",
        "  \"\"\"\n",
        "  sequence_to_fasta_path = {}\n",
        "  # Deduplicate to not do redundant work for multiple copies of the same chain in homomers.\n",
        "  for sequence_index, sequence in enumerate(sorted(set(sequences)), 1):\n",
        "    fasta_path = f'target_{sequence_index:02d}.fasta'\n",
        "    with open(fasta_path, 'wt') as f:\n",
        "      f.write(f'>query\\n{sequence}')\n",
        "    sequence_to_fasta_path[sequence] = fasta_path\n",
        "\n",
        "  # Run the search against chunks of genetic databases (since the genetic\n",
        "  # databases don't fit in Colab disk).\n",
        "  raw_msa_results = {sequence: {} for sequence in sequence_to_fasta_path.keys()}\n",
        "  print('\\nGetting MSA for all sequences')\n",
        "  with tqdm.notebook.tqdm(total=TOTAL_JACKHMMER_CHUNKS, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    def jackhmmer_chunk_callback(i):\n",
        "      pbar.update(n=1)\n",
        "\n",
        "    for db_config in MSA_DATABASES:\n",
        "      db_name = db_config['db_name']\n",
        "      pbar.set_description(f'Searching {db_name}')\n",
        "      jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=JACKHMMER_BINARY_PATH,\n",
        "          database_path=db_config['db_path'],\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=db_config['num_streamed_chunks'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=db_config['z_value'])\n",
        "      # Query all unique sequences against each chunk of the database to prevent\n",
        "      # redunantly fetching each chunk for each unique sequence.\n",
        "      results = jackhmmer_runner.query_multiple(list(sequence_to_fasta_path.values()))\n",
        "      for sequence, result_for_sequence in zip(sequence_to_fasta_path.keys(), results):\n",
        "        raw_msa_results[sequence][db_name] = result_for_sequence\n",
        "\n",
        "  return raw_msa_results\n",
        "\n",
        "\n",
        "features_for_chain = {}\n",
        "raw_msa_results_for_sequence = get_msa(sequences)\n",
        "for sequence_index, sequence in enumerate(sequences, start=1):\n",
        "  raw_msa_results = copy.deepcopy(raw_msa_results_for_sequence[sequence])\n",
        "\n",
        "  # Extract the MSAs from the Stockholm files.\n",
        "  # NB: deduplication happens later in pipeline.make_msa_features.\n",
        "  single_chain_msas = []\n",
        "  uniprot_msa = None\n",
        "  for db_name, db_results in raw_msa_results.items():\n",
        "    merged_msa = notebook_utils.merge_chunked_msa(\n",
        "        results=db_results, max_hits=MAX_HITS.get(db_name))\n",
        "    if merged_msa.sequences and db_name != 'uniprot':\n",
        "      single_chain_msas.append(merged_msa)\n",
        "      msa_size = len(set(merged_msa.sequences))\n",
        "      print(f'{msa_size} unique sequences found in {db_name} for sequence {sequence_index}')\n",
        "    elif merged_msa.sequences and db_name == 'uniprot':\n",
        "      uniprot_msa = merged_msa\n",
        "\n",
        "  notebook_utils.show_msa_info(single_chain_msas=single_chain_msas, sequence_index=sequence_index)\n",
        "\n",
        "  # Turn the raw data into model features.\n",
        "  feature_dict = {}\n",
        "  feature_dict.update(pipeline.make_sequence_features(\n",
        "      sequence=sequence, description='query', num_res=len(sequence)))\n",
        "  feature_dict.update(pipeline.make_msa_features(msas=single_chain_msas))\n",
        "  # We don't use templates in AlphaFold Colab notebook, add only empty placeholder features.\n",
        "  feature_dict.update(notebook_utils.empty_placeholder_template_features(\n",
        "      num_templates=0, num_res=len(sequence)))\n",
        "\n",
        "  # Construct the all_seq features only for heteromers, not homomers.\n",
        "  if model_type_to_use == ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "    valid_feats = msa_pairing.MSA_FEATURES + (\n",
        "        'msa_species_identifiers',\n",
        "    )\n",
        "    all_seq_features = {\n",
        "        f'{k}_all_seq': v for k, v in pipeline.make_msa_features([uniprot_msa]).items()\n",
        "        if k in valid_feats}\n",
        "    feature_dict.update(all_seq_features)\n",
        "\n",
        "  features_for_chain[protein.PDB_CHAIN_IDS[sequence_index - 1]] = feature_dict\n",
        "\n",
        "np_example = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
        "\n",
        "with open(os.path.join(output_dir, \"features.pkl\"), \"wb\") as f:\n",
        "  pickle.dump(np_example, f, protocol=4)\n",
        "\n",
        "executed_cells.add(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUo6foMQxwS2"
      },
      "outputs": [],
      "source": [
        "#@title 5. Run base AlphaFold prediction\n",
        "\n",
        "#@markdown Once this cell has been executed, a zip-archive with\n",
        "#@markdown the obtained prediction will be automatically downloaded\n",
        "#@markdown to your computer.\n",
        "\n",
        "#@markdown In case you are having issues with the relaxation stage, you can disable it below.\n",
        "#@markdown Warning: This means that the prediction might have distracting\n",
        "#@markdown small stereochemical violations.\n",
        "\n",
        "run_relax = False  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Relaxation is faster with a GPU, but we have found it to be less stable.\n",
        "#@markdown You may wish to enable GPU for higher performance, but if it doesn't\n",
        "#@markdown converge we suggested reverting to using without GPU.\n",
        "\n",
        "relax_use_gpu = False  #@param {type:\"boolean\"}\n",
        "\n",
        "# Track cell execution to ensure correct order\n",
        "notebook_utils.check_cell_execution_order(executed_cells, 5)\n",
        "\n",
        "# --- Run the model ---\n",
        "model_names = config.MODEL_PRESETS['monomer'] + ('model_2_ptm',)\n",
        "\n",
        "plddts = {}\n",
        "ranking_confidences = {}\n",
        "pae_outputs = {}\n",
        "unrelaxed_proteins = {}\n",
        "\n",
        "with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "  for model_name in model_names:\n",
        "    pbar.set_description(f'Running {model_name}')\n",
        "\n",
        "    cfg = config.model_config(model_name)\n",
        "\n",
        "    cfg.data.eval.num_ensemble = 1\n",
        "\n",
        "    params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "    model_runner = model.RunModel(cfg, params)\n",
        "    processed_feature_dict = model_runner.process_features(np_example, random_seed=0)\n",
        "    prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize))\n",
        "\n",
        "    mean_plddt = prediction['plddt'].mean()\n",
        "\n",
        "    if 'predicted_aligned_error' in prediction:\n",
        "      pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                   prediction['max_predicted_aligned_error'])\n",
        "    else:\n",
        "      # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they\n",
        "      # should never get selected.\n",
        "      ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "      plddts[model_name] = prediction['plddt']\n",
        "\n",
        "    # Set the b-factors to the per-residue plddt.\n",
        "    final_atom_mask = prediction['structure_module']['final_atom_mask']\n",
        "    b_factors = prediction['plddt'][:, None] * final_atom_mask\n",
        "    unrelaxed_protein = protein.from_prediction(\n",
        "        processed_feature_dict,\n",
        "        prediction,\n",
        "        b_factors=b_factors,\n",
        "        remove_leading_feature_dimension=(\n",
        "            model_type_to_use == ModelType.MONOMER))\n",
        "    unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "\n",
        "    # Delete unused outputs to save memory.\n",
        "    del model_runner\n",
        "    del params\n",
        "    del prediction\n",
        "    pbar.update(n=1)\n",
        "\n",
        "  # --- AMBER relax the best model ---\n",
        "\n",
        "  # Find the best model according to the mean pLDDT.\n",
        "  best_model_name = max(ranking_confidences.keys(), key=lambda x: ranking_confidences[x])\n",
        "\n",
        "  if run_relax:\n",
        "    pbar.set_description(f'AMBER relaxation')\n",
        "    amber_relaxer = relax.AmberRelaxation(\n",
        "        max_iterations=0,\n",
        "        tolerance=2.39,\n",
        "        stiffness=10.0,\n",
        "        exclude_residues=[],\n",
        "        max_outer_iterations=3,\n",
        "        use_gpu=relax_use_gpu)\n",
        "    relaxed_pdb, _, _ = amber_relaxer.process(prot=unrelaxed_proteins[best_model_name])\n",
        "  else:\n",
        "    print('Warning: Running without the relaxation stage.')\n",
        "    relaxed_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name])\n",
        "  pbar.update(n=1)  # Finished AMBER relax.\n",
        "\n",
        "# Construct multiclass b-factors to indicate confidence bands\n",
        "# 0=very low, 1=low, 2=confident, 3=very high\n",
        "banded_b_factors = []\n",
        "for plddt in plddts[best_model_name]:\n",
        "  for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
        "    if plddt >= min_val and plddt <= max_val:\n",
        "      banded_b_factors.append(idx)\n",
        "      break\n",
        "banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
        "to_visualize_pdb = utils.overwrite_b_factors(relaxed_pdb, banded_b_factors)\n",
        "\n",
        "\n",
        "# Write out the prediction\n",
        "pred_output_path = os.path.join(output_dir, 'selected_prediction.pdb')\n",
        "with open(pred_output_path, 'w') as f:\n",
        "  f.write(relaxed_pdb)\n",
        "\n",
        "\n",
        "# --- Visualise the prediction & confidence ---\n",
        "show_sidechains = True\n",
        "def plot_plddt_legend():\n",
        "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
        "  thresh = ['Very low (pLDDT < 50)',\n",
        "            'Low (70 > pLDDT > 50)',\n",
        "            'Confident (90 > pLDDT > 70)',\n",
        "            'Very high (pLDDT > 90)']\n",
        "\n",
        "  colors = [x[2] for x in PLDDT_BANDS]\n",
        "\n",
        "  plt.figure(figsize=(2, 2))\n",
        "  for c in colors:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  ax = plt.gca()\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['bottom'].set_visible(False)\n",
        "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
        "  return plt\n",
        "\n",
        "# Color the structure by per-residue pLDDT\n",
        "color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)}\n",
        "view = py3Dmol.view(width=800, height=600)\n",
        "view.addModelsAsFrames(to_visualize_pdb)\n",
        "style = {'cartoon': {'colorscheme': {'prop': 'b', 'map': color_map}}}\n",
        "if show_sidechains:\n",
        "  style['stick'] = {}\n",
        "view.setStyle({'model': -1}, style)\n",
        "view.zoomTo()\n",
        "\n",
        "grid = GridspecLayout(1, 2)\n",
        "out = Output()\n",
        "with out:\n",
        "  view.show()\n",
        "grid[0, 0] = out\n",
        "\n",
        "out = Output()\n",
        "with out:\n",
        "  plot_plddt_legend().show()\n",
        "grid[0, 1] = out\n",
        "\n",
        "display.display(grid)\n",
        "\n",
        "# Display pLDDT and predicted aligned error (if output by the model).\n",
        "if pae_outputs:\n",
        "  num_plots = 2\n",
        "else:\n",
        "  num_plots = 1\n",
        "\n",
        "plt.figure(figsize=[8 * num_plots, 6])\n",
        "plt.subplot(1, num_plots, 1)\n",
        "plt.plot(plddts[best_model_name])\n",
        "plt.title('Predicted LDDT')\n",
        "plt.xlabel('Residue')\n",
        "plt.ylabel('pLDDT')\n",
        "\n",
        "if num_plots == 2:\n",
        "  plt.subplot(1, 2, 2)\n",
        "  pae, max_pae = list(pae_outputs.values())[0]\n",
        "  plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
        "  plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "  # Display lines at chain boundaries.\n",
        "  best_unrelaxed_prot = unrelaxed_proteins[best_model_name]\n",
        "  total_num_res = best_unrelaxed_prot.residue_index.shape[-1]\n",
        "  chain_ids = best_unrelaxed_prot.chain_index\n",
        "  for chain_boundary in np.nonzero(chain_ids[:-1] - chain_ids[1:]):\n",
        "    if chain_boundary.size:\n",
        "      plt.plot([0, total_num_res], [chain_boundary, chain_boundary], color='red')\n",
        "      plt.plot([chain_boundary, chain_boundary], [0, total_num_res], color='red')\n",
        "\n",
        "  plt.title('Predicted Aligned Error')\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "\n",
        "# Save the predicted aligned error (if it exists).\n",
        "pae_output_path = os.path.join(output_dir, 'predicted_aligned_error.json')\n",
        "if pae_outputs:\n",
        "  # Save predicted aligned error in the same format as the AF EMBL DB.\n",
        "  pae_data = confidence.pae_json(pae=pae, max_pae=max_pae.item())\n",
        "  with open(pae_output_path, 'w') as f:\n",
        "    f.write(pae_data)\n",
        "\n",
        "executed_cells.add(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup KnotFold\n",
        "#####################################\n",
        "### Load things and function defs ###\n",
        "#####################################\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "from glob import glob\n",
        "import re\n",
        "\n",
        "from alphafold.data import templates\n",
        "from alphafold.common import protein\n",
        "from alphafold.common import residue_constants\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "from alphafold.model import model\n",
        "from alphafold.relax import relax\n",
        "\n",
        "_SUBSAMPLE_MSA_FEATURE_NAMES = [\n",
        "  'msa',\n",
        "  'deletion_matrix',\n",
        "  'msa_mask',\n",
        "  'msa_row_mask',\n",
        "  'bert_mask',\n",
        "  'true_msa',\n",
        "  'deletion_matrix_int',\n",
        "  'msa_species_identifiers',\n",
        "  'cluster_bias_mask'\n",
        "]\n",
        "_TRUNCATE_FEATURE_NAMES = [\"cluster_bias_mask\"]\n",
        "\n",
        "RELAX_MAX_ITERATIONS = 0\n",
        "RELAX_ENERGY_TOLERANCE = 2.39\n",
        "RELAX_STIFFNESS = 10.0\n",
        "RELAX_EXCLUDE_RESIDUES = []\n",
        "RELAX_MAX_OUTER_ITERATIONS = 3\n",
        "\n",
        "\n",
        "def mk_mock_template(lseq: str, multimer: bool) -> dict:\n",
        "\n",
        "  r\"\"\"Generates mock templates that will not influence prediction\n",
        "  Taken from ColabFold version 62d7558c91a9809712b022faf9d91d8b183c328c\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq: Query sequence\n",
        "  Returns\n",
        "  ----------\n",
        "  Dictionary with blank/empty/meaningless features\n",
        "  \"\"\"\n",
        "\n",
        "  # Define constants\n",
        "  # there are 37 atom types in alphafold (this number is 37)\n",
        "  lentype = templates.residue_constants.atom_type_num\n",
        "\n",
        "  # Since alphafold's model requires a template input\n",
        "  # We create a blank example w/ zero input, confidence -1\n",
        "  # templates function returns a numpy array the wrapper np.array is not needed\n",
        "  aatypes = np.array(\n",
        "    templates.residue_constants.sequence_to_onehot(\n",
        "      \"-\" * lseq, templates.residue_constants.HHBLITS_AA_TO_ID\n",
        "    )\n",
        "  )\n",
        "\n",
        "  if multimer:\n",
        "    return {\n",
        "      \"template_all_atom_positions\": np.zeros((lseq, lentype, 3))[None], #[None] adds another dimension\n",
        "      \"template_all_atom_mask\": np.zeros((lseq, lentype))[None],\n",
        "      \"template_aatype\": aatypes[None],\n",
        "    }\n",
        "  else:\n",
        "    return {\n",
        "      \"template_all_atom_positions\": np.zeros((lseq, lentype, 3))[None],\n",
        "      \"template_all_atom_masks\": np.zeros((lseq, lentype))[None],\n",
        "      \"template_sequence\": [f\"none\".encode()], # encode - efficient storage of strings\n",
        "      \"template_aatype\": aatypes[None],\n",
        "      \"template_confidence_scores\": np.full(lseq, -1)[None],\n",
        "      \"template_domain_names\": [f\"none\".encode()],\n",
        "      \"template_release_date\": [f\"none\".encode()],\n",
        "    }\n",
        "\n",
        "def partition_break(list_in, n):\n",
        "  random.shuffle(list_in) #shuffle the list of sequences\n",
        "  indices_sets = [list_in[i::n] for i in range(n)] #for the number of splits (3) get the shuffled sequences\n",
        "  return indices_sets\n",
        "\n",
        "def partition_Nalignments(list_in, n):\n",
        "  list_out = []\n",
        "  list_in = np.array(list_in)\n",
        "  length = len(list_in)\n",
        "  for i in range(n):\n",
        "    list_out.append(list(list_in+i*length))\n",
        "  return list_out\n",
        "\n",
        "def mutation_method(n,pairs,msa,mode='breakN'):\n",
        "  n_samples = msa.shape[0]\n",
        "  list_in = list(range(n_samples))\n",
        "  if mode=='breakN':\n",
        "    indices_sets = partition_break(list_in,n)\n",
        "  elif mode=='breakN+1':\n",
        "    indices_sets = partition_break(list_in,n+1)\n",
        "    flatpairs=[element for sublist in pairs for element in sublist]\n",
        "    pairs.append(flatpairs)\n",
        "\n",
        "  elif mode=='Nalignments':\n",
        "    msa = np.tile(msa,(n,1))\n",
        "    indices_sets = partition_Nalignments(list_in,n)\n",
        "\n",
        "  return indices_sets, pairs, msa\n",
        "\n",
        "\n",
        "def back_to_cysteine(msa, msa_index, cysteines,pair):\n",
        "  for c in pair:\n",
        "    msa[msa_index, cysteines[c-1]] = 1\n",
        "  return msa\n",
        "\n",
        "def replace_cystines(msa, cysteines, mode='hydrophobic'):\n",
        "  if mode=='all':\n",
        "    for c in cysteines:\n",
        "      msa[:, c] = random.choices([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19], k=len(msa[:, c])) #ACDEFGHIKLMNPQRSTVWY\n",
        "  elif mode=='hydrophobic':\n",
        "    for c in cysteines:\n",
        "      msa[:, c] = random.choices([0,1,4,7,9,10,17,18,19], k=len(msa[:, c])) #ACFILMVWY\n",
        "      #replace the cystines with a random amino acid (numbers corespond to AF mapping)\n",
        "  elif mode=='small':\n",
        "    for c in cysteines:\n",
        "      msa[:, c] = random.choices([0,1,9,15,16,17], k=len(msa[:, c])) #ACLSTV\n",
        "  else:\n",
        "      raise Exception('Mode of cystine replacement needs to be properly specified')\n",
        "  return msa"
      ],
      "metadata": {
        "id": "Sy6m1iA50ivJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Run KnotFold prediction\n",
        "\n",
        "\n",
        "####################\n",
        "### INPUT VALUES ###\n",
        "####################\n",
        "\n",
        "cysteine_pairs = '12,34,56' # @param {type:\"string\"}\n",
        "num_preds_per_model = 3 # @param {type:\"integer\"}\n",
        "aa_option = 'all' # @param ['all', 'hydrophobic', 'small'] {allow-input: false}\n",
        "mutation_option = 'breakN' # @param ['breakN', 'Nalignments', 'breakN+1'] {allow-input: false}\n",
        "amber_relax = \"False\" # @param ['True', 'False']\n",
        "gpu_relax = \"False\" # @param ['True', 'False']\n",
        "templated = False # @param ['False', 'True']\n",
        "\n",
        "str_pairs = cysteine_pairs.replace(',','_')\n",
        "\n",
        "####################\n",
        "### Run KnotFold ###\n",
        "####################\n",
        "\n",
        "with open(os.path.join(output_dir, \"features.pkl\"), \"rb\") as f: #load features (come from an alphafold run)\n",
        "  features = pickle.load(f)\n",
        "sequence = features['sequence'][0].decode()\n",
        "cysteines = [m.start() for m in re.finditer('C', sequence)]\n",
        "pairs = []\n",
        "for i,p in enumerate(str_pairs.split(\"_\")):\n",
        "  pair = []\n",
        "  for j,value in enumerate(p):\n",
        "    pair.append(int(value))\n",
        "  pairs.append(pair)\n",
        "  print('zero indexed cysteine locations', cysteines)\n",
        "  print('pairs to be forced', pairs)\n",
        "\n",
        "#######################\n",
        "### ADJUST FEATURES ###\n",
        "#######################\n",
        "\n",
        "msa = features[\"msa\"]\n",
        "print(type(msa))\n",
        "print(msa.shape)\n",
        "indices_sets, pairs, msa = mutation_method(len(pairs), pairs, msa, mutation_option)\n",
        "\n",
        "msa = replace_cystines(msa, cysteines, aa_option)\n",
        "for i, pair in enumerate(pairs): #with zero indexing turn the split up cystines back into cystines in the pairs\n",
        "  msa = back_to_cysteine(msa, indices_sets[i], cysteines, pair)\n",
        "features[\"msa\"] = msa\n",
        "\n",
        "if mutation_option=='Nalignments':\n",
        "  features['msa_species_identifiers'] = np.tile(features['msa_species_identifiers'],(1,len(pairs)))\n",
        "  features['num_alignments']=features['num_alignments']*len(pairs)\n",
        "  features['deletion_matrix_int'] = np.tile(features['deletion_matrix_int'],(len(pairs),1))\n",
        "\n",
        "#if not templated:\n",
        "#  lseq = features[\"template_all_atom_positions\"].shape[1]\n",
        "#  template_features = mk_mock_template(lseq, multimer=False)\n",
        "#  features.update(template_features)\n",
        "\n",
        "with open(os.path.join(output_dir, \"updated_features.pkl\"), \"wb\") as f:\n",
        "  pickle.dump(features, f, protocol=4)\n",
        "\n",
        "######################\n",
        "### RUN PREDICTION ###\n",
        "######################\n",
        "\n",
        "knotfold_plddts = {}\n",
        "knotfold_ranking_confidences = {}\n",
        "knotfold_pae_outputs = {}\n",
        "knotfold_unrelaxed_proteins = {}\n",
        "\n",
        "\n",
        "with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "  for model_name in model_names:\n",
        "    pbar.set_description(f'Running {model_name}')\n",
        "\n",
        "    cfg = config.model_config(model_name)\n",
        "\n",
        "    cfg.data.eval.num_ensemble = 1\n",
        "\n",
        "    params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "    model_runner = model.RunModel(cfg, params)\n",
        "    processed_feature_dict = model_runner.process_features(features, random_seed=0)\n",
        "    prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize))\n",
        "\n",
        "    mean_plddt = prediction['plddt'].mean()\n",
        "\n",
        "    if 'predicted_aligned_error' in prediction:\n",
        "      knotfold_pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                  prediction['max_predicted_aligned_error'])\n",
        "    else:\n",
        "      # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they\n",
        "      # should never get selected.\n",
        "      knotfold_ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "      knotfold_plddts[model_name] = prediction['plddt']\n",
        "\n",
        "    # Set the b-factors to the per-residue plddt.\n",
        "    final_atom_mask = prediction['structure_module']['final_atom_mask']\n",
        "    b_factors = prediction['plddt'][:, None] * final_atom_mask\n",
        "    unrelaxed_protein = protein.from_prediction(\n",
        "      processed_feature_dict,\n",
        "      prediction,\n",
        "      b_factors=b_factors,\n",
        "      remove_leading_feature_dimension=(\n",
        "        model_type_to_use == ModelType.MONOMER))\n",
        "    knotfold_unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "\n",
        "    # Delete unused outputs to save memory.\n",
        "    del model_runner\n",
        "    del params\n",
        "    del prediction\n",
        "    pbar.update(n=1)\n",
        "\n",
        "  # --- AMBER relax the best model ---\n",
        "\n",
        "  # Find the best model according to the mean pLDDT.\n",
        "  knotfold_best_model_name = max(knotfold_ranking_confidences.keys(), key=lambda x: knotfold_ranking_confidences[x])\n",
        "\n",
        "  if run_relax:\n",
        "    pbar.set_description(f'AMBER relaxation')\n",
        "    amber_relaxer = relax.AmberRelaxation(\n",
        "        max_iterations=0,\n",
        "        tolerance=2.39,\n",
        "        stiffness=10.0,\n",
        "        exclude_residues=[],\n",
        "        max_outer_iterations=3,\n",
        "        use_gpu=relax_use_gpu)\n",
        "    relaxed_pdb, _, _ = amber_relaxer.process(prot=knotfold_unrelaxed_proteins[knotfold_best_model_name])\n",
        "  else:\n",
        "    print('Warning: Running without the relaxation stage.')\n",
        "    relaxed_pdb = protein.to_pdb(knotfold_unrelaxed_proteins[knotfold_best_model_name])\n",
        "  pbar.update(n=1)  # Finished AMBER relax.\n",
        "\n",
        "# Construct multiclass b-factors to indicate confidence bands\n",
        "# 0=very low, 1=low, 2=confident, 3=very high\n",
        "banded_b_factors = []\n",
        "for plddt in plddts[best_model_name]:\n",
        "  for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
        "    if plddt >= min_val and plddt <= max_val:\n",
        "      banded_b_factors.append(idx)\n",
        "      break\n",
        "banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
        "to_visualize_pdb = utils.overwrite_b_factors(relaxed_pdb, banded_b_factors)\n",
        "\n",
        "# Write out the prediction\n",
        "pred_output_path = os.path.join(output_dir, f\"knotfold_{str_pairs}_selected_prediction.pdb\")\n",
        "with open(pred_output_path, 'w') as f:\n",
        "  f.write(relaxed_pdb)\n",
        "\n",
        "\n",
        "# --- Visualise the prediction & confidence ---\n",
        "show_sidechains = True\n",
        "def plot_plddt_legend():\n",
        "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
        "  thresh = ['Very low (pLDDT < 50)',\n",
        "            'Low (70 > pLDDT > 50)',\n",
        "            'Confident (90 > pLDDT > 70)',\n",
        "            'Very high (pLDDT > 90)']\n",
        "\n",
        "  colors = [x[2] for x in PLDDT_BANDS]\n",
        "\n",
        "  plt.figure(figsize=(2, 2))\n",
        "  for c in colors:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  ax = plt.gca()\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['bottom'].set_visible(False)\n",
        "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
        "  return plt\n",
        "\n",
        "# Color the structure by per-residue pLDDT\n",
        "color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)}\n",
        "view = py3Dmol.view(width=800, height=600)\n",
        "view.addModelsAsFrames(to_visualize_pdb)\n",
        "style = {'cartoon': {'colorscheme': {'prop': 'b', 'map': color_map}}}\n",
        "if show_sidechains:\n",
        "  style['stick'] = {}\n",
        "view.setStyle({'model': -1}, style)\n",
        "view.zoomTo()\n",
        "\n",
        "grid = GridspecLayout(1, 2)\n",
        "out = Output()\n",
        "with out:\n",
        "  view.show()\n",
        "grid[0, 0] = out\n",
        "\n",
        "out = Output()\n",
        "with out:\n",
        "  plot_plddt_legend().show()\n",
        "grid[0, 1] = out\n",
        "\n",
        "display.display(grid)\n",
        "\n",
        "# Display pLDDT and predicted aligned error (if output by the model).\n",
        "if pae_outputs:\n",
        "  num_plots = 2\n",
        "else:\n",
        "  num_plots = 1\n",
        "\n",
        "plt.figure(figsize=[8 * num_plots, 6])\n",
        "plt.subplot(1, num_plots, 1)\n",
        "plt.plot(plddts[best_model_name])\n",
        "plt.title('Predicted LDDT')\n",
        "plt.xlabel('Residue')\n",
        "plt.ylabel('pLDDT')\n",
        "\n",
        "if num_plots == 2:\n",
        "  plt.subplot(1, 2, 2)\n",
        "  pae, max_pae = list(knotfold_pae_outputs.values())[0]\n",
        "  plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
        "  plt.colorbar(fraction=0.046, pad=0.04)\n",
        "\n",
        "  # Display lines at chain boundaries.\n",
        "  best_unrelaxed_prot = knotfold_unrelaxed_proteins[knotfold_best_model_name]\n",
        "  total_num_res = best_unrelaxed_prot.residue_index.shape[-1]\n",
        "  chain_ids = best_unrelaxed_prot.chain_index\n",
        "  for chain_boundary in np.nonzero(chain_ids[:-1] - chain_ids[1:]):\n",
        "    if chain_boundary.size:\n",
        "      plt.plot([0, total_num_res], [chain_boundary, chain_boundary], color='red')\n",
        "      plt.plot([chain_boundary, chain_boundary], [0, total_num_res], color='red')\n",
        "\n",
        "  plt.title('Predicted Aligned Error')\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "\n",
        "# Save the predicted aligned error (if it exists).\n",
        "pae_output_path = os.path.join(output_dir, f\"knotfold_{str_pairs}_predicted_aligned_error.json\")\n",
        "if pae_outputs:\n",
        "  # Save predicted aligned error in the same format as the AF EMBL DB.\n",
        "  pae_data = confidence.pae_json(pae=pae, max_pae=max_pae.item())\n",
        "  with open(pae_output_path, 'w') as f:\n",
        "    f.write(pae_data)\n",
        "\n",
        "# --- Download the predictions ---\n",
        "shutil.make_archive(base_name='prediction', format='zip', root_dir=output_dir)\n",
        "files.download(f'{output_dir}.zip')\n",
        "\n",
        "executed_cells.add(6)"
      ],
      "metadata": {
        "id": "MM1Ycqxz_tTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfPhvYgKC81B"
      },
      "source": [
        "# License\n",
        "\n",
        "AlphaFold Code and Model Parameters License follow the terms stated at: https://github.com/google-deepmind/alphafold"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}